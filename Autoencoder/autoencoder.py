
import numpy
import math
import time
import scipy.io
import scipy.optimize
import matplotlib.pyplot

#from visualization import *
import os
import pickle

MODELS_DIR = r'.'


###########################################################################################
""" The Autoencoder base class """

class Autoencoder(object):

    #######################################################################################
    """ Initialization of Autoencoder object """

    def __init__(self, visible_size, hidden_size, lamda, ae_name = 'autoencoder'): #rho, beta can be added by the sparse autoencoder
    
        """ Initialize parameters of the Autoencoder object """

        self.ae_name = ae_name # this will be used to save the pickle of this model
    
        self.visible_size = visible_size    # number of input units
        self.hidden_size = hidden_size      # number of hidden units
        self.lamda = lamda                  # weight decay parameter
        
        """ Set limits for accessing 'theta' values """
        
        self.limit0 = 0
        self.limit1 = hidden_size * visible_size
        self.limit2 = 2 * hidden_size * visible_size
        self.limit3 = 2 * hidden_size * visible_size + hidden_size
        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size
        
        """ Initialize Neural Network weights randomly
            W1, W2 values are chosen in the range [-r, r] """

        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1)
        print 'vs = ', visible_size, ' hs = ', hidden_size, '  r = ', r
        
        rand = numpy.random.RandomState(int(time.time()))
        
        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))
        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))
        
        print "W1"
        print W1
        print "W2"
        print W2
        
        """ Bias values are initialized to zero """
        
        b1 = numpy.zeros((hidden_size, 1))
        b2 = numpy.zeros((visible_size, 1))
        
        print b1
        
        print W1.shape

        """ Create 'theta' by unrolling W1, W2, b1, b2 """
		
        print 'type w1 = ', type(W1.flatten()), 'type b1 = ', type(b1.flatten()), 'w1 shape = ', W1.flatten().shape, ' b1 shape = ', b1.flatten().shape

        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),
                                        b1.flatten(), b2.flatten()))
        
        self.final_theta = self.theta #we will place the final theta generated by the model here
        self.final_theta_hidden = numpy.concatenate((W1.flatten(), b1.flatten()))
        self.iteration_count = 0


    #######################################################################################
    """ Returns elementwise sigmoid output of input array """
    
    def sigmoid(self, x):
    
        return (1 / (1 + numpy.exp(-x)))

    #######################################################################################
    """ Returns the cost of the Autoencoder and gradient at a particular 'theta' """
        
    def compute_cost(self, theta, input):
        
        """ Extract weights and biases from 'theta' input """
        
        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)
        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)

        #NOTE: W2 is needed only for reconstruction - for stacking it is not needed
        
        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)
        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)
        
        """ Compute output layers by performing a feedforward pass
            Computation is done for all the training inputs simultaneously """

        self.iteration_count += 1

        if (self.iteration_count % 1) == 0:
            print 'in iteration: ', self.iteration_count

        # W1 is |h| x |v| matrix, input is |v| x m matrix where m = number of training examples
        # b1 is |h| vector

        #print W1.shape, input.shape

        W1x = numpy.dot(W1, input)

        #print 'w1x shape = ', W1x.shape, 'w1x+b1 shape = ', numpy.asarray((W1x + b1[0])).shape
        #print W1x, '-' * 50, '\n', (W1x + b1[0]), '-' * 50, '\n', b1
        
        hidden_layer = self.sigmoid(W1x + b1) #b1[0])

        #print 'Shapes are: ', W1.shape, input.shape, b1.shape, hidden_layer.shape, ' w2 shape = ', W2.shape, ' b2 shape = ', b2.shape
        
        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2) #b2[0])
        
        """ Estimate the average activation value of the hidden layers """
        
        """ Compute intermediate difference values using Backpropagation algorithm """

        diff = output_layer - input
        #print 'out shape = ', output_layer.shape, '  in shape = ', input.shape, ' diff shape = ', diff.shape

        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1]
        #weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) + numpy.sum(numpy.multiply(W2, W2))) #original
        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) + numpy.sum(numpy.multiply(W2, W2))) / input.shape[1]

        cost                 = sum_of_squares_error + weight_decay # + KL_divergence


        #-------------------------------- COMPUTE log based cost ---------------------------------
        '''
        log_output_layer = numpy.log(output_layer)

        #print 'log htheta = ', log_output_layer
        
        log_1_output_layer = numpy.log(1 - output_layer) # need to check/confirm if 1 - log x broadcasts properly in numpy

        #print 'log (1 - htheta) = ', log_1_output_layer

        logdiff = (input * log_output_layer) + ((1 - input) * log_1_output_layer)
        #print 'logdiff = ', logdiff
        
        sum_logdiff_error = -1 * (numpy.sum(logdiff) / input.shape[1])
        #print 'err = ', sum_logdiff_error

        cost_1 = sum_logdiff_error + weight_decay

        #print 'COST_1 = ', cost_1
        '''
        
        #-----------------------------------------------------------------------------------------        
        
        
        #print diff.shape, sum_of_squares_error.shape, weight_decay.shape

        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))
        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out),
                                 numpy.multiply(hidden_layer, 1 - hidden_layer))
        
        #print del_out.shape, del_hid.shape

        """ Compute the gradient values by averaging partial derivatives
            Partial derivatives are averaged over all training examples """
            
        W1_grad = numpy.dot(del_hid, numpy.transpose(input))
        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))
        b1_grad = numpy.sum(del_hid, axis = 1)
        b2_grad = numpy.sum(del_out, axis = 1)
            
        W1_grad = W1_grad / input.shape[1] + self.lamda * W1
        W2_grad = W2_grad / input.shape[1] + self.lamda * W2
        b1_grad = b1_grad / input.shape[1]
        b2_grad = b2_grad / input.shape[1]
        
        """ Transform numpy matrices into arrays """
        
        W1_grad = numpy.array(W1_grad)
        W2_grad = numpy.array(W2_grad)
        b1_grad = numpy.array(b1_grad)
        b2_grad = numpy.array(b2_grad)
        
        """ Unroll the gradient values and return as 'theta' gradient """
        
        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),
                                        b1_grad.flatten(), b2_grad.flatten()))
        print 'AE_COST = ', cost
        return [cost, theta_grad]
        #return [cost_1, theta_grad]

    def train(self, training_data, max_iterations = 400):
        print 'Starting AE Training size = ', training_data.shape

        # original
        model  = scipy.optimize.minimize(self.compute_cost, self.theta, 
                                            args = (training_data,), method = 'L-BFGS-B', 
                                            jac = True, options = {'maxiter': max_iterations})


        self.final_theta = model.x

        #need to extract the hidden layer parameters from here
        self.final_theta_hidden = numpy.concatenate((self.final_theta[self.limit0 : self.limit1], self.final_theta[self.limit2 : self.limit3]))
        fpath = os.path.join(MODELS_DIR, self.ae_name) + '.p'
        print 'finished training, saving the model at: ', fpath
        pickle.dump(self, open(fpath, "wb"))
        return model

    #For stacking the autoencoders we need to connect the output of the hidden units to the next AE
    #the function below computes and returns the hidden activations when layer = 'hidden'
    #if the layer = 'output' the output layer is computed and returned
        
    def predict(self, input_vector, layer = 'output'): #given the input vector 1 x n predict the output which ideally should be same as input
        
        """ Extract weights and biases from 'theta' input """
        theta = self.final_theta
        
        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)
        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)
        
        """ Compute output layers by performing a feedforward pass
            Computation is done for all the training inputs simultaneously """
        
        hidden_layer = self.sigmoid(numpy.dot(W1, input_vector) + b1)

        #print 'From predict, Shapes are: ', W1.shape, input_vector.shape, b1.shape, hidden_layer.shape

        if layer == 'hidden':
            return hidden_layer

        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)
        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)
        
        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2)
        
        #print 'From predict, Out Shapes are: ', W2.shape, hidden_layer.shape, b2.shape, output_layer.shape
        #print input_vector.shape, hidden_layer.shape, output_layer.shape, W1.shape, b1.shape

        return output_layer

    def visualize(self, layer = None):
        if layer == None: #default is to visualize the hidden layer
            weights = self.final_theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)
            b1 = self.final_theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)
        display_image(weights)
        return weights

    def compute_numerical_grads(self, theta, tdata):
        epsilon = 0.0001
        #W1_flat = list(theta[self.limit0 : self.limit1]) #.reshape(self.hidden_size, self.visible_size)
        W1_flat = list(theta) #.reshape(self.hidden_size, self.visible_size)
        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)

        grad = []

        print 'len w1 = ', len(W1_flat)
        
        for i in range(len(W1_flat)): # for all weights do the numeric check
            W1_e1 = list(W1_flat)
            W1_e2 = list(W1_flat)
            W1_e1[i] = W1_flat[i] + epsilon
            W1_e2[i] = W1_flat[i] - epsilon

            # we now have prepared 2 weight vectors representing the perturbation
            # let us find the cost for each and compute grad values
            #theta[self.limit0 : self.limit1] = W1_e1
            theta1 = numpy.array(W1_e1)

            #print 'theta = ', theta
            
            cost1 = self.compute_cost(theta1, tdata)[0] #[self.limit0 : self.limit1]
            #theta[self.limit0 : self.limit1] = W1_e2
            theta1 = numpy.array(W1_e2)
            cost2 = self.compute_cost(theta1, tdata)[0] #[self.limit0 : self.limit1]
            #print 'costs = ', (cost1), (cost2)
            #deltas = [cost1[i] - cost2[i] for i in range(len(cost1))]
            deltas = cost1 - cost2
            #print len(deltas)
            
            grad.append(deltas / (2.0 * epsilon))
        return grad

    def gradient_check(self, X): #
        self.lamda = 0
        cost, grad = self.compute_cost(self.theta, X)
        grad1 = self.compute_numerical_grads(self.theta, X)
        #grad = grad[self.limit0 : self.limit1]
        return {'cost': cost, 'grad': grad, 'numeric_grad': grad1}
        
        
